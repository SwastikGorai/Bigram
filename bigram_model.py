# -*- coding: utf-8 -*-
"""Bigram_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zW9DP9-V6nJ5yoCpTI7_54XlwL1f7-3U
"""

import tensorflow as tf
import numpy as np

names = open('names .txt','r').read().splitlines()

# names[:5]

minn = min(len(w) for w in names)
maxx = max(len(w) for w in names)
print(f'Min and max len of names : {minn} and {maxx}')

bigram_set = {}
for i in names:
  # print(i)
  x = ['<S>'] + list(i) + ['<E>']
  for y,z in zip(x, x[1:]):
    bigram = (y,z)
    bigram_set[bigram] = bigram_set.get(bigram, 0) + 1

# bigram_set

chars = sorted(list(set(''.join(names))))
# chars
stoi = {s:i+1 for i,s in enumerate(chars)}
stoi['.'] = 0
itos = {i:s for s,i in stoi.items()}
NxN = len(itos)
# itos

bigram_set_matrix = np.zeros((NxN,NxN), dtype = int)

for w in names:
  chs = ['.'] + list(w) + ['.']
  for ch1, ch2 in zip(chs, chs[1:]):
    i1 = stoi[ch1]
    i2 = stoi[ch2]
    bigram_set_matrix[i1, i2] += 1
bigram_set_matrix = tf.Variable(bigram_set_matrix, dtype = tf.int32)

# bigram_set_matrix[20][21]

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
def plotMxM(NxN, itos, bigram_set_matrix ):
  plt.figure(figsize=(28,28))
  plt.imshow(bigram_set_matrix, cmap='Blues')
  for i in range(NxN):
      for j in range(NxN):
          chstr = itos[i] + itos[j]
          plt.text(j, i, chstr, ha="center", va="bottom", color='black')
          plt.text(j, i, int(bigram_set_matrix[i, j]), ha="center", va="top", color='black')
  # plt.patch.set_edgecolor('black')
  # plt.patch.set_linewidth(1)
  plt.axis('off');
  plt.title('Matrix of Bigrams')



plotMxM(NxN, itos, bigram_set_matrix)

# bigram_set_matrix[0]

#Normalising and adding smoothening(+1 to all cells)


bigram_set_matrix = tf.add(bigram_set_matrix, 1)

bigram_set_matrix.shape

bigram_set_matrix = tf.cast(bigram_set_matrix, dtype = tf.float32)

summ = tf.reduce_sum(bigram_set_matrix, axis = 1, keepdims=True)  # Req is [56,1]
# summ.shape
# summ

ProbDiv = bigram_set_matrix / summ
ProbTFDiv = tf.divide(bigram_set_matrix, summ)

print(ProbDiv.shape)
tf.reduce_sum(ProbDiv[0])

print(ProbTFDiv.shape)
print(tf.reduce_sum(ProbTFDiv[0])) # Should be 1

# ProbTFDiv[0]

tf.random.set_seed(2147483647)
g = tf.random.Generator.from_seed(2147483647)
for i in range(5):
  out = []
  ix = 0
  while True:
    p = ProbTFDiv[ix]
    # print(p)
    # ix = tf.random.categorical(p[tf.newaxis], num_samples = 1, dtype = tf.int32 ).numpy()[0][0]
    num_samples=1
    replacement=True
    p_tf = tf.constant(p, dtype=tf.float32)
    ix = tf.random.categorical(tf.math.log([p_tf]), num_samples=num_samples)
    ix = tf.squeeze(ix).numpy()
    #__
    out.append(itos[ix])
    if ix == 0:
      break
  print(''.join(out))

# GOAL:
# maximize likelihood of the data w.r.t. model parameters (statistical modeling)
# equivalent to maximizing the log likelihood (because log is monotonic)
# equivalent to minimizing the negative log likelihood
# equivalent to minimizing the average negative log likelihood
# less is better
# log(a*b*c) = log(a) + log(b) + log(c)


log_likelihood = 0.0
n = 0

for w in names:

   chs = ['.'] + list(w) + ['.']
   for ch1, ch2 in zip(chs, chs[1:]):
    ix1 = stoi[ch1]
    ix2 = stoi[ch2]
    prob = ProbTFDiv[ix1, ix2]
    logprob = tf.math.log(prob)
    log_likelihood += logprob
    n += 1
    #print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')

print(f'{log_likelihood=}')
# Negative Log Likelihood
nll = -log_likelihood
print(f'Negative log likelihood :{nll=}')
print(f'Average negative log likelihood: {nll/n}')

# Just visualising
xs, ys = [], []

for w in names[:1]:
  chs = ['.'] + list(w) + ['.']
  for ch1, ch2 in zip(chs, chs[1:]):
    ix1 = stoi[ch1]
    ix2 = stoi[ch2]
    print(ch1, ch2)
    xs.append(ix1)
    ys.append(ix2)

print(xs)
print(ys)

Encoded = tf.one_hot(xs, dtype = tf.float32, depth = len(itos) )
# Encoded

# Encoded.shape

plt.imshow(Encoded)

weight_matrix = tf.random.normal((27,27)) # 27 rows, 27 column
print(weight_matrix)
logits = Encoded @ weight_matrix
print(logits)

# Log-scale counts to counts by using exponentiation
count = tf.exp(logits)
print(count)

# calculating the sum of elements along the second dimension (axis 1) of the tensor for normalisation:
count_sum = tf.reduce_sum(count, axis=1, keepdims=True)
print(count_sum)

probablity_dist = count/count_sum
# probablity_dist

tf.reduce_sum(probablity_dist[0]) # Should be 1

